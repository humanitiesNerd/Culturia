<html><head><meta charset="utf-8" /><title>guile-culturia</title><link rel="stylesheet" href="static/normalize.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Gentium+Basic" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans" /><link rel="stylesheet" href="static/main.css" /></head><body><h1><a href="//hyperdev.fr/projects/culturia">hyperdev.fr/projects/culturia</a></h1><div id="container"><p><strong>OpenCog Sneak Peek</strong></p><p>Main goal: simplify <em>local cognitive load</em>, the rule is to :</p><ul><li>avoid opening doors</li><li>focus and what are the directions of the project.</li></ul><p>Move to an more algorithm, software centric overview while keeping the
background theory.</p><p>Notes taking algorithm:</p><ul><li><p>global: keep the intent ie. overview of opencog with research grounds</p><ul><li><p>rework the document hierarchy to make more sens (FIXME)</p></li><li><p>keep the research background but make it less proeminent</p></li><li><p>move all the uncertainity and research work to a specific section (FIXME)</p></li></ul></li><li><p>local:</p><ul><li><p>simplify wording aka. avoid words I don&apos;t know. I&apos;m sorry Yeats. Globish FTW!</p></li><li><p>simplify phrasing aka. less prose more bullets</p></li><li><p>avoid adjectives</p></li><li><p>avoid “because X, Y we should Z”  and instead “we must Z”</p></li></ul></li><li><p>glocal (key concept that forward to more general facts)</p><ul><li><p>do not cite thing with no prior proper introduction (except glocal)</p></li><li><p>less implementation purity facts like &quot;MOSES handles <em>much of</em> procedural
learning&quot; prefer &quot;MOSES handle procedural learning&quot;</p></li><li><p>less contextualisation like &quot;computationally feasible&quot;, &quot;mainstream in IA&quot; etc..</p></li><li><p>focus contextually relevant facts instead of trying to be comprehensive</p></li><li><p>strip examples [FIXME: add examples]</p></li></ul></li></ul><p><strong>The goal is to move to this table of content</strong></p><ul><li><p>Introduction</p><ul><li><p>AI vs AGI</p></li><li><p>{Open,Cog,Prime}</p></li><li><p>Secret Sauce</p></li><li><p>Key Claims</p></li></ul></li><li><p>Theory (Pattern Theory of the Mind)</p></li><li><p>Implementation</p><ul><li>AtomSpace</li><li>Natural Language Pipeline</li><li>MOSES</li><li>DeSTIN</li><li>PLN</li><li>...</li></ul></li><li><p>Applications</p></li><li><p>Roadmap</p></li></ul><h1>Introduction</h1><h2>AI vs AGI</h2><p>AI is really narrow-AI. AGI also know as <em>Human Level Intelligence</em> is a revival
of original AI goals to create a human-like intelligence.</p><h2>{Open, Cog, Prime}</h2><ul><li>OpenCog is a framework</li><li>CogPrime is an AGI</li><li>OpenCogPrime is the implementation of CogPrime using OpenCog (proprietary)</li></ul><h2>What Kind of Intelligence is OpenCog Aimed At?</h2><p>Pratical goals:</p><ul><li><em>Turing Test</em>: the classic, involving passing as a human being in an everyday
conversation</li><li><em>Virtual World</em> or <em>Telerobotic Turing Test</em> : pass as a human being, in a
conversation involving controlling a virtual world avatar or a robot,
alongside verbal interaction</li><li><em>Online University Test</em>: attend an online university just like a human
student, and graduate</li><li><em>Physical University Test</em>: control a robot that attends university just
like a human student, and graduates</li><li><em>Artificial Scientist Test</em>: write and publish original science papers,
based on ideas conceived by the AI due to its own reading of the literature</li></ul><p>[There is also an argument about “ressources” and “limited ressources”.
I&apos;ll paraphrase  Alan Kay argument “To build the software of the future
use the ressources of the future”
[https://www.youtube.com/watch?v=gTAghAJcO1o](The Future doesn&apos;t have to be incremental)]</p><h2>What&apos;s the secret sauce</h2><p><em>Secret sauce</em>: Making diverse AI algorithms with diverse background theories
work together in a <em>synergetic</em> and <em>cooperative</em> way.</p><p><em>white boxes</em> instead of <em>black boxes</em>.</p><p>Another important aspect of intelligence is <em>adaptiveness</em>: the capability of a
system to adapt to its environment [recognize similar problems and apply
previous structure and algorithm] effectively.</p><p>A key principle is: the use of multiple cognitive processes
associated with multiple types of memory to enable an intelligent agent
to execute the procedures that it believes have the best probability of
working toward its goals in its current context.</p><p>There is no particular new algorithm or architecture principle.</p><ul><li><em>synergy</em>: the interaction of elements that when combined produce a total
effect that is greater than the sum of the individual elements, contributions,
etc.</li></ul><h2>Key Claims</h2><p>This is a list of claims that the reader must accept to validate that OpenCog
approach to AGI is a viable one.</p><p>An AGI system must:</p><ul><li><p>Use much of its <em>resources</em></p></li><li><p>Represent different kind of knowledge via different kinds of <em>cognitive memory</em>:</p><ul><li><p>declarative</p></li><li><p>procedural</p></li><li><p>episodic</p></li><li><p>sensory</p></li><li><p>intentional</p></li><li><p>attentional</p></li></ul></li><li><p>Use <em>cognitive synergy</em> between different cognitive process to overcome
processing bottlenecks. This means that cognitive processus must be able to
understand and use the memory of each other.</p></li><li><p>have a global, local and <em>glocal</em> memory</p></li><li><p>have sensory data and motoric affordances that roughly emulate those
available to humans.</p></li><li><p>must grow/evolve up along a path roughly comparable to that followed by human
children. It is not human intelligence man, it learns human intelligence.</p></li><li><p>learning must happend using a mix of spontaneous learning and explicit
instricution:</p><ul><li>imitation</li><li>reinforcement and correction</li><li>linguistic and nonlinguistic instruction</li></ul></li><li><p>learn human language via built-in NLP facility that can be improved based on
experiences. NLP facility must be both rule and statistic based.</p></li><li><p>recognize and represent large-scale pattern in itself called the <em>cognitive equation</em>
[reference: Chaotic Logic. Plenum, 1994. BibTeX [Goe94]].</p></li><li><p>upon sustained interaction with an environment in pursuit of goals emerge
internal knowledge network, including but not limited to:</p><ul><li><em>hierarchical network: representing both a spatiotemporal hierarchy and an
approximate “default inheritance” hierarchy, cross-linked</em></li><li><a href="https://en.wikipedia.org/wiki/Heterarchy">heterarchical network</a> <em>of
associativity, roughly aligned with the hierarchical network</em></li><li><em>self network which is an approximate micro image of the whole network</em></li><li><em>inter-reflecting networks modeling self and others, reflecting a
“mirrorhouse” design pattern</em>
[reference: Mirror Neurons, Mirrorhouses, and the Algebraic Structure of the Self GASP08].</li></ul></li><li><p>implement a simplicity bias in each cognitive process cf. <a href="https://en.wikipedia.org/wiki/Occam&apos;s_razor">Occam’s Razor</a>.</p></li><li><p>if supplied with a commonsensically ethical goal system and an intentional
component based on rigorous uncertain inference, should be able to reliably
achieve a much higher level of commonsensically ethical behavior than any
human being.</p></li><li><p>once sufficiently advanced, an AGI system with a logic-based declarative
knowledge approach and a program-learning-based procedural knowledge approach
should be able to radically self-improve via a variety of methods, including
supercompilation and automated theorem-proving.</p></li></ul><p>Efficiency is not a side-issue but rather the essence of real-world AGI
[reference: Goertzel, Ben. Toward a Formal Definition of Real-World General
Intelligence.].
Hutter has shown, if one casts efficiency aside, arbitrary levels of general
intelligence can be achieved via a trivially simple program</p><p>[In the original article this was a point in the above list, this goes a bit
further in the definition of what an AGI must do and instead dive into how
it should do it, describing all the components of OpenCog]</p><p>Given the strengths and weaknesses of current and near-future digital computers:</p><ul><li><em>a (loosely) neural-symbolic network is a good representation for
directly storing many kinds of memory, and interfacing between those
that it doesn’t store directly;</em></li><li><em>Uncertain logic is a good way to handle declarative knowledge. To
deal with the problems facing a human-level AGI, an uncertain logic
must integrate imprecise probability and fuzziness with a broad
scope of logical
constructs. <a href="http://wiki.opencog.org/wikihome/index.php/Probabilistic_Logic_Networks">PLN</a>
is one good realization.</em></li><li><em>Programs are a good way to represent procedures (both cognitive and
physical-action, but perhaps not including low-level motor-control
procedures).</em></li><li><em>Evolutionary program learning is a good way to handle difficult
program learning problems. Probabilistic learning on normalized
programs is one effective approach to evolutionary program
learning. <a href="http://wiki.opencog.org/wikihome/index.php/Meta-Optimizing_Semantic_Evolutionary_Search">MOSES</a>
is one good realization of this approach.</em></li><li>Multistart hill-climbing, with a strong Occam prior, is a good way
to handle relatively straightforward program learning problems.</li><li><p>Activation spreading and Hebbian learning comprise a reasonable way
to handle attentional knowledge (though other approaches, with
greater overhead cost, may provide better accuracy and may be
appropriate in some situations).</p><ul><li>Artificial economics is an effective approach to activation
spreading and Hebbian learning in the context of neural-symbolic
networks;</li><li>ECAN is one good realization of artificial economics;</li><li>A good trade-off between comprehensiveness and efficiency is to
focus on two kinds of attention: processor attention (represented
in CogPrime by ShortTermImportance) and memory attention
(represented in CogPrime by LongTermImportance).</li></ul></li><li>Simulation is a good way to handle episodic knowledge (remembered
and imagined). Running an internal world simulation engine is an
effective way to handle simulation.</li><li>Hybridization of one’s integrative neural-symbolic system with a
spatiotemporally hierarchical deep learning system is an effective
way to handle representation and learning of low-level sensorimotor
knowledge. DeSTIN is one example of a deep learning system of this
nature that can be effective in this context.</li><li>One effective way to handle goals is to represent them
declaratively, and allocate attention among them
economically. CogPrime ’s PLN/ECAN based framework for handling
intentional knowledge is one good realization.</li></ul><p>[Missing reference to <a href="http://wiki.opencog.org/wikihome/index.php/RelEx_Dependency_Relationship_Extractor">Relex</a> and GrammarLink]</p><h1>Theory</h1><h2>Pattern based theory of the mind</h2><p>OpenCog way of thinking about intelligent systems is guided by the patternist
[as in <em>pattern</em>] philosophy of the mind.</p><p>It&apos;s based on the premise that mind is made of pattern – and that a mind is a
system for recognizing patterns in itself and the world. An important kind of
pattern include patterns which <strong>identify which procedures are likely to lead to
the achievement of which goals in which contexts</strong>.</p><h3>What is a pattern?</h3><p>In the patternist philosophy, &quot;pattern&quot; is generally defined as &quot;representation
as something simpler.&quot; Thus, for example, if one measures simplicity in terms
of bit-count, then a program compressing an image would be a pattern of that
image. But if one uses a simplicity measure incorporating run-time as well as
bit-count, then the compressed version may or may not be a pattern of the image,
depending on how one’s simplicity measure weights of the two factors.</p><p>[replaced &quot;in&quot; by &quot;of&quot; in for exemple &quot;of the image&quot;]</p><p><em>Pattern</em> definition includes simple repeated patterns and fractal patterns.</p><p>While pattern theory comes from computational theory, it is not tied to
computation; it can be developed in any context where there is a notion of
&quot;representation&quot; or &quot;production&quot; and a way of measuring simplicity</p><p>[skipped <em>pattern intensity</em> formula]</p><h3>What is intelligence?</h3><p>In patternism the intelligent system is conceived as the (fuzzy) set of
patterns in that system, and the set of patterns emergent between that system
and other systems with which it interacts. The latter means that the patternist
perspective is inclusive of notions of distributed intelligence.</p><p>A mind is a collection of patterns that is associated with a persistent
dynamical process that achieves highly-patterned goals in highly-patterned
environments.</p><p>[XXX: What are the pattern I want to solve?]</p><p>Also in patternist philosophy of mind, reflection is critical to intelligence.</p><h3>Which patterns?</h3><p>Patternism gives a structure to the tasks synthesizing intelligent systems.
We are led to ask questions such as:</p><ul><li>How are patterns represented in the system?</li><li>How does the underlying infrastructure of the system give rise to the displaying of a particular pattern in the system’s behavior?</li><li>What kinds of patterns are most compactly represented within the system?</li><li>What kinds of patterns are most simply learned?</li><li>What learning processes are utilized for recognizing patterns?</li><li>What mechanisms are used to give the system the ability to introspect (so that it can recognize patterns in itself)?</li></ul><p>[Implicit in the patternist theory patterns are graph patterns]</p><p>These same questions can be asked for “knowledge” or “information”. However, we
have found that asking these questions in the context of pattern leads to more
productive answers, avoiding unproductive byways and also tying in very nicely
with the details of various existing formalisms and algorithms for knowledge
representation and learning.</p><p>Among other kinds of patterns in intelligent systems, [semiotic patterns]()
are very interesting. [Peirce]() decomposed these into three categories:</p><ul><li><em>iconic patterns</em>, which are patterns of contextually important internal
similarity between two entities (e.g. an iconic pattern binds a picture of
a person to that person)</li><li><em>indexical patterns</em>, which are patterns of spatiotemporal co-occurrence
(e.g. an indexical pattern binds a wedding dress and a wedding)</li><li><em>symbolic patterns</em>, which are patterns indicating that two entities are often
involved in the same relationships (e.g. a symbolic pattern between the number
“5” (the symbol) and various sets of 5 objects (the entities that the symbol
is taken to represent))</li></ul><p><em>symbolic patterns</em> have played an especially large role in the history of AI.</p><p>Mathematical logic and related formalisms provide sophisticated mechanisms for
combining and relating symbolic patterns. Some AI approaches have focused heavily
on these, more so than on <strong>the identification of symbolic patterns in experience
or the use of them to achieve practical goals</strong>.</p><h3>How are organized those patterns?</h3><p>Following from the view of intelligence in terms of achieving complex goals in
complex environments, comes a view in which the dynamics of a cognitive system
are understood to be governed by two main forces:</p><ul><li><em>self-organization</em>, via which system dynamics cause existing system patterns
to give rise to new ones</li><li><em>goal-oriented behavior</em>, basically a system interacting with its environment
in a way to maximize some function.</li></ul><p>Both are cooperative aspects.</p><p>[... posited ...]</p><p>[I don&apos;t understand the link between the previous paragraph a the paragraph I
skipped and the following paragraph]</p><p>[This introduce the “playing with blocks” scenario that is developed through
the book where a virtual agent in minecraft-like word is given the task
to impress some other agent with its creativity]</p><p>The principles [This explains more or less the same thing, lake of structure]:</p><ul><li><em>Evolution</em> [pattern query] process via which patterns within a large
population thereof are selected and used as the basis for formation of new
patterns, based on some “fitness function” that is generally tied to
the goals of the agent.</li><li><em>Autopoiesis</em> [pattern synergy] process via which a system of interrelated
patterns maintains its integrity, whenever one of the patterns in the system
begins to decrease in intensity, some of the other patterns increase their
intensity which causes the troubled pattern to increase in intensity again.</li><li><em>Association</em> [pattern cooperation] when patterns given attention they spread
some of this attention to patterns they have previously been associated with.
Furthermore there is Peirce’s <em>law of mind</em>, stating that the mind is an
<em>associative memory network</em>, whose dynamics dictate that ideas of active
agent, continually act on those ideas with which the memory associates
[looks like this is related to Hebbian thing].</li><li><em>Differential attention allocation / credit assignment</em> [pattern cooperation]
Patterns that have been valuable for goal-achievement are given more
attention, and are encouraged to participate in giving rise to new patterns.</li><li><em>Pattern creation</em> Patterns that have been valuable for goal-achievement are
mutated and combined with each other to yield new patterns. [same as above
but the goal is stressed].</li></ul><p>The network of patterns give rise to the following structures:</p><ul><li><p><em>Hierarchical network</em> patterns are relations of control over other patterns
that represent more specialized aspects of themselves.</p></li><li><p><em>Heterarchical network</em>. The system retains a memory of which patterns have
previously been associated with each other in any way.</p></li><li><p><em>Dual network</em> Hierarchical and heterarchical structures are combined, with
the dynamics of the two structures working together harmoniously. Among many
possible ways to hierarchically organize a set of patterns, the one used
should be one that causes hierarchically nearby patterns to have many
meaningful heterarchical connections; and of course, there should be a
tendency to search for heterarchical connections among hierarchically nearby
patterns.</p></li><li><p><em>Self structure</em>. A portion of the network of patterns forms into an
approximate image of the overall network of patterns.</p></li></ul><p>The success of CogPrime will depend on whether these high-level structures and
dynamics can be made to emerge from the synergetic interaction of CogPrime&apos;s
representation and algorithms.</p><p><a href="http://goertzel.org/monkeyburger/bbm_main.pdf">Building Better Minds: An Architecture for Artificial General Intelligence. In preparation, 2013.</a> elaborate how these concepts arises concretely from CogPrime&apos;s structures and algorithms.</p><h2>Mind-World Correspondence Principle</h2><p>An additional philosophical principle has guided CogPrime design; this is
the &quot;mind-world correspondence principle&quot;, which enlarges on the notion of
&quot;intelligence as adaptation to environments&quot;.</p><p>For intelligence to occur, there has to be a natural correspondence between
the transition-sequences of world-states and the corresponding
transition-sequences of mind-states, at least in the cases of
transition-sequences leading to relevant goals.</p><h2>High-Level Architecture of CogPrime</h2><p><a href="http://wiki.opencog.org/wikihome/index.php/File:1-no_gray.jpg">Figure depecting the different cognitive process with interactions</a>.</p><h2>Local, Global and Glocal</h2><p>The two major supercategories of knowledge representation systems are <em>local</em> also called <em>explicit</em> and <em>global</em> also called <em>implicit</em> systems, with a hybrid category <em>glocal</em> that combines both local and global (explicit and implicit).</p><p>In CogPrime all three are realized using the same network AtomSpace.</p><p>In the following we discuss:</p><ul><li>the symbolic, semantic-network aspects of knowledge representation</li><li>distributed, neural-net-like knowledge representation, focusing on CogPrime’s
<em>glocal</em> knowledge representation</li></ul><h1>OpenCog framework</h1><p>As described above OpenCog is an opionated framework which takes its root
in the following principles:</p><ul><li>Patternist theory of the mind</li><li>Expert cognitive processus</li><li>Synergy between processus</li></ul><h2>Natural Language Processing Pipeline</h2><p>[Nothing about atoms until this but cited]
[This part is not at its place]</p><p>The idea is to start with a <em>seed of rule-based NLP system</em> that will evolve
can evolve using OpenCog learning systems. The NLP system is both rule and
statistical based.</p><p>The NLP pipeline is described by the following diagram:</p><pre><code>[Link Parser] -&gt; [ReLex] -&gt; [Link2Atom] -&gt; {AtomSpace}</code></pre><p>A combination of rule-based and statistical NLP tools have been integrated with
OpenCog, translating English into OpenCog atoms and vice versa:</p><ul><li><p><a href="http://www.abisource.com/projects/link-grammar/">Link Parser</a>: maps English
sentences into parse structures. Variants exists:</p><ul><li><p>SAT link parser (based on Boolean satisfaction solver)</p></li><li><p>Viterbi link parser (try to closely simulate the human reading process)</p></li></ul></li><li><p><a href="http://wiki.opencog.org/wikihome/index.php/RelEx_Dependency_Relationship_Extractor">RelEx</a>:
translates Link Parser output into more abstract syntactico-semantic parses.
Also handles some other odds and ends like anaphor resolution.</p></li><li><p><a href="http://wiki.opencog.org/wikihome/index.php/Link2Atom">Link2Atom</a>: translates
RelEx output to OpenCog atoms</p></li></ul><p>Also, NLGen2 has the task to build english sentend based on the results of the
above pipeline.</p><h2>AtomSpace</h2><p>Weigthed, Labeled Hypergraph.</p><p>There are different knowledge representations in AI systems in an explicit, localized way, most of them descending from formal logic.</p><p>CogPrime&apos;s explicit localized knowledge representation use a generalized
hypergraph, a [graph]() consisting of:</p><ul><li>Node that can embed hypergraphs</li><li>Link that can connect more that two nodes and can connect to other links</li></ul><p>In OpenCog, Node and Link inherit the Atom type.</p><p><em>generalized hypergraph</em> are always refered as simply <em>hypergraph</em>.</p><p>[XXX: need an example of the above in opencog]</p><p>[XXX: is it possible to link a hypernode in an embedded hypergraph to and
hypernode outside the embedded hypergraph]</p><p>[XXX: In culturia, atoms are really hyper-links and there is no hyper-nodes.
It might be possible to implement embedded hypergraph on top. hierarchical
hypergraph doesn&apos;t count as embedded hypergraph]</p><p>Atoms comes with type, name and weights.</p><p>An hypergraph comes with the following dynamics:</p><ul><li>modify the properties of atoms such as the labels or weights</li><li>add new atoms</li><li>remove existing ones</li></ul><h3>Atoms Types and Weights</h3><p>It is important to note that the AtomSpace is neither a <em>neural net</em> nor a
<em>semantic net</em>.</p><p>It is not a <em>neural net</em> because it has no activation values, and involves no
attempts at low-level brain modeling. However, attention values are very loosely
analogous to time-averages of neural net activations.</p><p>It is not a <em>semantic net</em> because of the broad scope of the Atoms in the
network: for example, Atoms may represent percepts, procedures, or parts of
concepts.</p><p>Most CogPrime Atoms have no corresponding English label. However, most CogPrime
Atoms do have probabilistic truth values, allowing logical semantics.</p><p>[XXX: This changed to Generic Truth Values]</p><p>Atoms can be quantified with <em>truth values</em> that can have two components, one
representing <em>probability</em> (strength) and the other representing
<em>weight of evidence</em>. There is also <em>attention values</em> that have two components,
<em>short-term</em> and <em>long-term importance</em>.</p><h4>Atoms</h4><h5>GetLink, PutLink and BindLink interaction</h5><p>https://groups.google.com/forum/?hl=fr#!topic/opencog/jXzPY7sJeUM</p><h3>Glocal Memory</h3><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview#Glocal_Memory">reference</a></p><p><em>Glocal coordination of local and global memory</em>.</p><p><em>Glocal memory</em> applies to many forms of memory; We will focus on perceptual and
declarative memory.</p><p>[XXX: I assume AtomSpace stores perceptual memory too, but what else?]</p><p>The central idea of <em>glocal memory</em> is that perceptual, declarative, episodic,
procedural, etc. items can be stored in memory in the form of paired structures
that are called <code>(key, map)</code> pairs.</p><p>The <code>key</code> is a localized version of the item, and records some significant
aspects of the items in a simple and crisp way. [It&apos;s a pattern for the items]</p><p>The <code>map</code> is a dispersed, distributed version of the item, which represents the
item as a (to some extent, dynamically shifting) combination of fragments of
other items.</p><p><em>glocality</em> lies at the heart of this combination:</p><ul><li><p><em>Local knowledge</em> ie. the <code>key</code> is represented in abstract logical
relationships stored in explicit logical form, and also in [Hebbian]()-type
associations between atoms</p></li><li><p><em>Global knowledge</em> is represented in large-scale patterns of atoms
weights, which lead to large-scale patterns of network activity, which often
take the form of attractors qualitatively similar to [Hopfield net attractors]().
These attractors are called maps.</p></li></ul><p>Activation of either one of the two tends to also activate the other one.</p><p>[XXX: <em>glocality</em> involves building/compiling local representations of global
knowledge that is relevant to the local memory serving as a <code>key</code> for
the global knowledge in the considered context. The <code>key</code> is a locally relevant,
activation <em>pattern</em> for a global knowledge. The difference between the
glocal construction and a regular <code>map</code> or <code>hashmap</code> is that the the glocal
<code>key</code> is part of the bigger memory it&apos;s refering to so it can be activated as
part of this memory too: That&apos;s why the <code>key</code> can be activated by the global
memory. For instance a «cat» memory can contain two keys
«lol cat from the interwebs» and «wild cats» both remain in the same global
memory of «cats» but have their identity specialized and linked to other
memory.]</p><h3>Memory Types and Associated Cognitive Processes in CogPrime</h3><p>We dig deeper into the internals of the CogPrime approach, turning to aspects
of the <em>relationship between structure and dynamics</em>.</p><p>CogPrime&apos;s memory types are:</p><ul><li>declarative,</li><li>procedural,</li><li>sensory,</li><li>episodic</li><li>attentional memory for allocating system resources generically,</li><li>intentional memory for allocating system resources in a goal-directed way.</li></ul><p>In terms of patternist cognitive theory, the multiple types of memory in
CogPrime should be considered as specialized ways of storing particular types
of pattern, optimized for spacetime efficiency.</p><p>[Like explained in the &quot;What&apos;s the Secret Sauce&quot;], the gist of CogPrime
architecture is <em>cooperation</em> and <em>cognitive synergy</em>.</p><p><em>Cognitive synergy</em> is implemented by the ability given to process to convert
their memory in the terms of other process memory.</p><h4>Cognitive Synergy in Probabilistic Logic Network</h4><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview#Cognitive_Synergy_in_PLN">reference</a></p><p>Let&apos;s elaborate on the role it plays in the interaction between <em>procedural and
declarative learning</em>:</p><ul><li>MOSES handle procedural learning</li><li>CogPrime simulation engine handle <em>episodic knowledge</em> [XXX: memory?]</li><li>Probabilistic Logic Networks (PLN), an <em>uncertain inference framework</em>,
handles declarative knowledge</li></ul><p>PLN seeks to achieve efficient inference control via integration with other
cognitive processes.</p><p>As a logic, PLN is broadly integrative: it combines certain term logic rules
with more standard predicate logic rules, and utilizes both fuzzy truth values
and a variant of imprecise probabilities called indefinite probabilities. PLN
mathematics tells how these uncertain truth values propagate through its logic
rules, so that uncertain premises give rise to conclusions with reasonably
accurately estimated uncertainty values.</p><p>PLN can be used in either forward or backward chaining mode; and in the language
introduced above, it can be used for either analysis or synthesis.</p><p>The combinatorial explosion of inference control is combatted by defering to other
cognitive processes when the inference control procedure is unable to make
a sufficiently confident choice of which inference steps to take next.</p><p>MOSES may rely on PLN to model its evolving populations of procedures, PLN may
rely on MOSES to create complex knowledge about the terms in its logical
implications. This is just one example of the multiple ways in which the
different cognitive processes in CogPrime interact synergetically.</p><h3>Goal-Oriented Dynamics</h3><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview#Goal-Oriented_Dynamics_in_CogPrime">reference</a></p><p>CogPrime’s dynamics has both goal-oriented and “spontaneous” aspects. [XXX:
should be introduction]</p><h3>Clarifying the Key Claims</h3><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview#Clarifying_the_Key_Claims">reference</a></p><h4>Mutli-Memory Systems</h4><p>It’s important that an AGI system can handle different kinds of memory (declarative, procedural, episodic, sensory, intentional, attentional) in customized but interoperable ways.</p><p>[XXX: because of space-time efficiency]</p><p>In cases where the same representational mechanism is used for different types
of knowledge, different cognitive processes are used, and often different
aspects of the representation (e.g. attentional knowledge is dealt with largely
by ECAN acting on AttentionValues and HebbianLinks in the AtomSpace; whereas
declarative knowledge is dealt with largely by PLN acting on TruthValues and
logical links, also in the AtomSpace).</p><p>In fact the multi-memory approach may have a broader importance, even to
intelligences without multimodal communication.</p><p>Decades of computer science and narrow-AI practice strongly suggest that
the“ one memory structure fits all” approach is not capable of leading
to effective real-world approaches.</p><h4>Preception, Action and Environment</h4><h4>Developmental Pathways</h4><p>One important case of learning that human children are particularly
good at is language learning; It is very tempting to give AGI systems
a “short cut” to language proficiency via making use of existing
rule-based and statistical-corpus-analysis-based NLP systems;</p><h4>Knowledge Representation</h4><p>The key goal for a knowledge representation for AGI should be
naturalness with respect to the AGI’s cognitive processes – i.e. the
cognitive processes shouldn’t need to undergo complex transformative
gymnastics to get information in and out of the knowledge
representation in order to do their cognitive work.</p><p>A neural-symbolic network is a good representation for directly
storing many kinds of memory, and interfacing between those that it
doesn’t store directly AtomSpace is a neural-symbolic network designed
to work nicely with PLN, MOSES, ECAN and the other key CogPrime
cognitive processes; it supplies them with what they need without
causing them undue complexities.</p><h4>Cognitive Processes</h4><p>The crux of intelligence is dynamics, learning, adaptation;</p><p>Given CogPrime’s multi-memory design, it’s natural to consider CogPrime’s
cognitive processes in terms of which memory subsystems they focus on even if
some cognitive process spans multiple memory.</p><h5>Uncertain Logic for Declarative Knowledge</h5><p>The PLN logic framework is one way of integrating imprecise
probability and fuzziness in a logical formalism that encompasses a
broad scope of logical constructs.</p><p>It integrates term logic and predicate logic</p><h5>Program Learning for Procedural Knowledge</h5><p>In designing CogPrime , we have acted based on the understanding that
programs are a good way to represent procedures – including both
cognitive and physical-action procedures, but perhaps not including
low-level motor-control procedures.</p><p>Using a special language called Combo that is essentially a minor
variant of LISP. What differentiates this use of LISP from many
traditional uses of LISP in AI is that we are only using the LISP-ish
representational style for procedural knowledge, rather than trying to
use it for everything.</p><p>Low level procedures are represented using DeSTIN.</p><p>With both it&apos;s possible to make a Combo program that  invoke DeSTIN procedures,
and encode higher-level actions like “pick up the cup in front of you slowly and
quietly, then hand it to Jim who is standing next to you.”</p><p>Having committed to use programs to represent many procedures, the next
question is how to learn programs:</p><ul><li>for straightforward learning problems, <em>hillclimbing with random restart</em> and a
<em>strong Occam bias</em> is an effective method</li><li>for other problems, <em>probabilistic evolutionary program learning</em> (MOSES) is
an effective method</li></ul><h5>Attention Allocation</h5><ul><li>Activation spreading is a reasonable way to handle attentional knowledge.</li><li>Hebbian learning as one route of learning associative relationships (with more
sophisticated methods such as information-geometric ones potentially also
playing a role)</li></ul><p>Where CogPrime differs from standard practice is in the use of an economic
metaphor to regulate activation spreading.</p><p>In the context of a neural-symbolic network, artificial economics is an
effective approach to activation spreading; and CogPrime’s ECAN framework seeks
to embody this idea.</p><p>One major choice made in the CogPrime design is to focus on two kinds
of attention: processor (represented by ShortTermImportance) and
memory (represented by LongTermImportance)</p><h5>Internal Simulation and Episodic Knowledge</h5><p>Simulation is a good way to handle episodic knowledge; and running an
internal “world simulation engine” is an effective way to handle
simulation.</p><h5>Low-Level Perception and Action</h5><p>Hybridization of one’s integrative neural-symbolic system with a
spatiotemporally hierarchical deep learning system is an effective way
to handle representation and learning of low-level sensorimotor
knowledge. DeSTIN is one example of a deep learning system of this
nature that can be effective in this context.</p><h5>Goals</h5><p>Given that we have characterized general intelligence as “the ability
to achieve complex goals in complex environments,” However, we have
chosen not to create a separate subsystem for intentional knowledge,
and instead have concluded that one effective way to handle goals is
to represent them declaratively, and allocate attention among them
economically.</p><p>Goals and subgoals are related using logical links as interpreted and
manipulated by PLN, and attention is allocated among goals using the
STI dynamics of ECAN, and a specialized variant.</p><p>Thus the mechanics of goal management is handled using uncertain
inference and artificial economics, whereas the figuring-out of how to
achieve goals is done integratively, relying heavily on procedural and
episodic knowledge as well as PLN and ECAN.</p><p>The combination of ECAN and PLN seems to overcome the well-known
shortcomings found with purely neural-net or purely inferential
approaches to goals. Neural net approaches generally have trouble with
abstraction, whereas logical approaches are generally poor at
real-time responsiveness and at tuning their details quantitatively
based on experience.</p><h5>Fulfilling the Cognitive Equation</h5><p>It is important for an intelligent system to have some way of
recognizing large-scale patterns in itself, and then embodying these
patterns as new, localized knowledge items in its memory.</p><p>This dynamic introduces a feedback dynamic between emergent pattern and
substrate, which is critical to general intelligence. It also ties in nicely
with the notion of “glocal memory” – essentially positing a localization of
some global memories, which naturally will result in the formation of some
glocal memories. One of the key ideas underlying the CogPrime design is that
given the use of a neural-symbolic network for knowledge representation, a
graph-mining based “map formation” heuristic is one good way to do this.</p><p>Map formation seeks to fulfill the Cognitive Equation directly. Rather than
relying on other cognitive processes to implicitly recognize overall system
patterns and embody them in the system as localized memories (though this
implicit recognition may also happen), the MapFormation MindAgent explicitly
carries out this process. Mostly this is done using fairly crude greedy pattern
mining heuristics, though if really subtle and important patterns seem to be
there, more sophisticated methods like evolutionary pattern mining may also be
invoked.</p><h5>Occam&apos;s Razor</h5><p>This quest for simplicity is present in many places throughout the
CogPrime design, for instance</p><ul><li>In MOSES and hillclimbing, where program compactness is an explicit
component of program tree fitness</li><li>In PLN, where the backward and forward chainers explicitly favor
shorter proof chains, and intensional inference explicitly
characterizes entities in terms of their patterns (where patterns
are defined as compact characterizations)</li><li>In pattern mining heuristics, which search for compact characterizations of data</li><li>In the forgetting mechanism, which seeks the smallest set of Atoms
that will allow the regeneration of a larger set of useful Atoms via
modestly-expensive application of cognitive processes</li><li>Via the encapsulation of procedural and declarative knowledge in
simulations, which in many cases provide a vastly compacted form of
storing real-world experiences</li></ul><p>Like cognitive synergy and emergent networks, Occam’s Razor is not
something that is implemented in a single place in the CogPrime
design, but rather an overall design principle that underlies nearly
every part of the system.</p><h5>Cognitive Synergy</h5><p>These synergies are absolutely critical to the proposed functionality
of the CogPrime system.  Without them, the cognitive mechanisms are
not going to work adequately well, but are rather going to succumb to
combinatorial explosions.</p><p>The other aspects of CogPrime - the cognitive architecture, the knowledge
representation, the embodiment framework and associated developmental teaching
methodology - are all critical as well, but none of these will yield the
critical emergence of intelligence without cognitive mechanisms that effectively
scale.</p><p>And, in the absence of cognitive mechanisms that effectively scale on their own,
we must rely on cognitive mechanisms that effectively help each other to scale.</p><h6>Synergies that Help Inference</h6><p>The combinatorial explosion in PLN is obvious: forward and backward chaining
inference are both fundamentally explosive processes, reined in only by pruning
heuristics.</p><p>For nontrivial complex inferences to occur, one needs really, really clever
pruning heuristics.</p><p>The CogPrime design combines simple heuristics with pattern mining, MOSES and
economic attention allocation as pruning heuristics. Economic attention
allocation assigns importance levels to Atoms, which helps guide pruning.
Greedy pattern mining is used to search for patterns in the stored corpus of
inference trees, to see if there are any that can be used as analogies for the
current inference.</p><p>And MOSES comes in when there is not enough information (from importance levels
or prior inference history) to make a choice, yet exploring a wide variety of
available options is unrealistic.</p><h6>Synergies that Help MOSES</h6><p>MOSES’s combinatorial explosion is obvious: the number of possible
programs of size N increases very rapidly with N. The only way to get
around this is to utilize prior knowledge, and as much as possible of
it. When solving a particular problem, the search for new solutions
must make use of prior candidate solutions evaluated for that problem,
and also prior candidate solutions (including successful and
unsuccessful ones) evaluated for other related problems.</p><p>But, extrapolation of this kind is in essence a contextual analogical
inference problem. In some cases it can be solved via fairly
straightforward pattern mining; but in subtler cases it will require
inference of the type provided by PLN. Also, attention allocation
plays a role in figuring out, for a given problem A, which problems B
are likely to have the property that candidate solutions for B are
useful information when looking for better solutions for A.</p><h5>Synergies that Help Attention Allocation</h5><p>Economic attention allocation, without help from other cognitive
processes, is just a very simple process analogous to “activation
spreading” and “Hebbian learning” in a neural network. The other
cognitive processes are the things that allow it to more sensitively
understand the attentional relationships between different knowledge
items (e.g. which sorts of items are often usefully thought about in
the same context, and in which order).</p><h5>Further Synergies Related to Pattern Mining</h5><p>[Further Synergies Related to Pattern Mining]</p><p>Statistical, greedy pattern mining is a simple process, but it
nevertheless can be biased in various ways by other, more subtle
processes.</p><p>For instance, if one has learned a population of programs via MOSES,
addressing some particular fitness function, then one can study which
items tend to be utilized in the same programs in this population. One
may then direct pattern mining to find patterns combining these items
found to be in the MOSES population. And conversely, relationships
denoted by pattern mining may be used to probabilistically bias the
models used within MOSES.</p><p>Statistical pattern mining may also help PLN by supplying it with
information to work on. For instance, conjunctive pattern mining finds
conjunctions of items, which may then be combined with each other
using PLN, leading to the formation of more complex predicates. These
conjunctions may also be fed to MOSES as part of an initial population
for solving a relevant problem.</p><p>Finally, the main interaction between pattern mining and MOSES/PLN is
that the former may recognize patterns in links created by the
latter. These patterns may then be fed back into MOSES and PLN as
data. This virtuous cycle allows pattern mining and the other, more
expensive cognitive processes to guide each other. Attention
allocation also gets into the game, by guiding statistical pattern
mining and telling it which terms (and which combinations) to spend
more time on.</p><h5>Synergies Related to Map Formation</h5><p>[Synergies Related to Map Formation]</p><p>The essential synergy regarding map formation is obvious: Maps are
formed based on the HebbianLinks created via PLN and simpler
attentional dynamics, which are based on which Atoms are usefully used
together, which is based on the dynamics of the cognitive processes
doing the “using.” On the other hand, once maps are formed and
encapsulated, they feed into these other cognitive processes. This
synergy in particular is critical to the emergence of self and
attention.</p><p>What has to happen, for map formation to work well, is that the
cognitive processes must utilize encapsulated maps in a way that gives
rise overall to relatively clear clusters in the network of
HebbianLinks. This will happen if the encapsulated maps are not too
complex for the system’s other learning operations to understand. So,
there must be useful coordinated attentional patterns whose
corresponding encapsulated-map Atoms are not too complicated. This has
to do with the system’s overall parameter settings, but largely with
the settings of the attention allocation component. For instance, this
is closely tied in with the limited size of “attentional focus” (the
famous 7 +/- 2 number associated with humans’ and other mammals short
term memory capacity). If only a small number of Atoms are typically
very important at a given point in time, then the maps formed by
grouping together all simultaneously highly important things will be
relatively small predicates, which will be easily reasoned about -
thus keeping the “virtuous cycle” of map formation and comprehension
going effectively.</p><h4>Emergent Structures and Dynamics</h4><p>We have spent much more time in this book on the engineering of
cognitive processes and structures, than on the cognitive processes
and structures that must emerge in an intelligent system for it to
display human-level AGI. However, this focus should not be taken to
represent a lack of appreciation for the importance of
emergence. Rather, it represents a practical focus: engineering is
what we must do to create a software system potentially capable of
AGI, and emergence is then what happens inside the engineered AGI to
allow it to achieve intelligence. Emergence must however be taken
carefully into account when deciding what to engineer!</p><p>One of the guiding ideas underlying the CogPrime design is that an AGI
system with adequate mechanisms for handling the key types of
knowledge mentioned above, and the capability to explicitly recognize
large-scale pattern in itself, should upon sustained interaction with
an appropriate environment in pursuit of appropriate goals, emerge a
variety of complex structures in its internal knowledge network,
including (but not limited to): a hierarchical network, representing
both a spatiotemporal hierarchy and an approximate “default
inheritance” hierarchy, cross-linked; a heterarchical network of
associativity, roughly aligned with the hierarchical network; a self
network which is an approximate micro image of the whole network; and
inter-reflecting networks modeling self and others, reflecting a
“mirrorhouse” design pattern.</p><p>The dependence of these posited emergences on the environment and
goals of the AGI system should not be underestimated. For instance,
PLN and pattern mining don’t have to lead to a hierarchical structured
AtomSpace. But if the AGI system is placed in an environment which it
hierarchically structured via its own efforts, thenPLN and pattern
mining very likely will lead to a hierarchically structured
AtomSpace. And if this environment consists of hierarchically
structured language and culture, then what one has is a system of
minds with hierarchical networks, each reinforcing the hierarchality
of each others’ networks. Similarly, integrated cognition doesn’t have
to lead to mirrorhouse structures, but integrated cognition about
situations involving other minds studying and predicting and judging
each other, is very likely to do so. What is needed for appropriate
emergent structures to arise in a mind, is mainly that the knowledge
representation is sufficiently flexible to allow these structures, and
the cognitive processes are sufficiently intelligent to observe these
structures in the environment and then mirror them internally. Of
course, it also doesn’t hurt if the internal structures and processes
are at least slightly biased toward the origination of the particular
high-level emergent structures that are characteristic of the system’s
environment/goals; and this is indeed the case with CogPrime
... biases toward hierarchical, heterarchical, dual and mirrorhouse
networks are woven throughout the system design, in a thoroughgoing
though not extremely systematic way.</p><h1>Applications</h1><ul><li>PLN + Relex: automated biological hypothesis generation based on information
gathered from PubMed abstracts</li><li>MOSES component for biological data analysis;</li><li>financial prediction, genetics, marketing data analysis and natural language
processing.</li></ul><p>Most relevant: OpenCog has also been used to control agents in virtual worlds</p><h2>Virtual Agents</h2><p><a href="deadlink://">OpenPetBrain</a> control virtual dog in a virtual world.</p><p>Functionalities includes:</p><ul><li>learning new behaviors based on imitation and reinforcement</li><li>responding to natural language commands and questions, with appropriate
actions and natural language replies</li><li>spontaneous exploration of their world, remembering their experiences and
using them to bias future learning and linguistic interaction</li></ul><p>Other tasks that could be implemented:</p><ul><li>Learning to build steps or ladders to get desired objects that are high up</li><li>Learning to build a shelter to protect itself from aggressors</li><li>Learning to build structures resembling structures that it&apos;s shown
(even if the available materials are a bit different)</li><li>Learning how to build bridges to cross chasms</li></ul><p>the AI significance of learning tasks like this depends on:</p><ul><li>what kind of feedback the system is given,</li><li>how complex its environment is</li></ul><p>The goal is to have the system learn to carry out tasks like this using general
learning mechanisms and a general cognitive architecture, based on embodied
experience and only scant feedback from human teachers.</p><p>Tasks of the project <em>at time of writing</em> include:</p><ul><li>Watch another character build steps to reach a high-up object</li><li>Figure out via imitation of this that, in a different context, building steps
to reach a high up object may be a good idea</li><li>Also figure out that, if it wants a certain high-up object but there are no
materials for building steps available, finding some other way to get elevated
will be a good idea that may help it get the object (including e.g. building a
ladder, or asking someone tall to pick it up, etc.)</li><li>Figure out that, if the character wants to hide its valued object from a
creature much larger than it, it should build a container with a small hole
that the character can get through, but the creature cannot.</li></ul><h2>Physical Robot</h2><p>Experiments were conducted using OpenCog and Nao Robot.</p><p>Interposing DeSTIN as a perception-&gt;action “black box” between OpenCog and a
robot can improve capability.</p><p>A a “white box” approach can lead to better results. This means  realtime use
of links between CogPrime nd DeSTIN internal networks.</p><h2>Build Me Something I Haven&apos;t Seen Before</h2><p>...</p><h1>Roadmap</h1><h2>Measuring Progress toward AGI</h2><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview#Measuring_Incremental_Progress_Toward_Human-Level_AGI">reference</a></p><h2>Technical Roadmap</h2><p>http://wiki.opencog.org/wikihome/index.php/OpenCogPrime:Roadmap
http://wiki.opencog.org/wikihome/index.php/Roadmap</p><h1>Glossary</h1><ul><li>AtomSpace: is a neural-symbolic network designed to work nicely with PLN, MOSES, ECAN and the other key CogPrime cognitive processes;</li><li>Atom</li><li>CogPrime</li><li>Cognitive synergy</li><li>Cognitive process</li><li>Declarative Learning</li><li>DeSTIN: http://wiki.opencog.org/wikihome/index.php/DeSTIN</li><li>LinkGrammar</li><li>Memory</li><li>Memory (declarative)</li><li>Memory (procedural)</li><li>Memory (sensory)</li><li>Memory (episodic)</li><li>Memory (attentional) for allocating system resources generically,</li><li>Memory (intentional) for allocating system resources in a goal-directed way.</li><li>MOSES: handle procedural learning</li><li>OpenCog</li><li>OpenCogPrime</li><li>patternist cognitive theory</li><li>Pattern</li><li>Probabilitstic Logic Network (PLN): uncertain inference framework</li><li>Procedural Learning</li><li>ReLex</li></ul><h1>Links</h1><ul><li><p><a href="http://wiki.opencog.org/wikihome/index.php/CogPrime_Overview">CogPrime Overview</a></p></li><li><p><a href="http://wiki.opencog.org/wikihome/index.php/Background_Publications">Bibliography of OpenCog</a></p></li></ul></div><p><small>cc-by-nc-sa</small></p></body></html>